---
title: "On the text analysis packages in R"
author: "Jiun Siew"
categories: ["R"]
tags: ["R", "text analytics"]
date: 2018-05-22
showDate: true
---



<p>I recently had the privelege of having to do some text analytics at work and
wanted to pen down some thoughts on what I found useful and what didn’t work so
well.</p>
<div id="data-cleaning" class="section level2">
<h2>Data Cleaning</h2>
<p>Cleaning free text data is by far, one of the hardest types of data that I’ve
had to deal with. In most textbooks/online tutorials, the corpus is usually
cleaned, making the document term matrix (DTM) creation fairly straightforward.
In the corpus that I had to work with, I found:</p>
<ol style="list-style-type: decimal">
<li>tons of spelling errors</li>
<li>acronyms and jargon that are domain specific</li>
<li>names, initials of names, and even nicknames</li>
</ol>
<p>The realities of trying to do some text analysis on really, really dirty data
means that extensive effort and thought is required to get the right set of
terms and vocabulary before any modelling can even begin.</p>
<p>For example, the corpus I was working with had different abbreviations for
the same string. One that I found was the term <code>owner occupier</code> which
was written as <code>o'occ</code>, <code>O/oc</code>, <code>own occ</code>, <code>owner occ</code>, <code>O/O</code>, <code>OO</code> and <code>oocc</code>.</p>
<p>While removing punctuation and forcing the text to lower case might have helped,
the DTM for the word <code>owner</code> and <code>occupier</code> would have effectively been missing
valuable counts.</p>
<div id="cleaning-bit-by-bit" class="section level3">
<h3>Cleaning, bit by bit</h3>
<p>After lots of trial and error, I eventually developed a process to clean the
data so that it provided relatively good modelling results and was reasonably
easy to automate. There were two main components to this process:</p>
<ul>
<li>replacing abbreviations, jargon with appropriate terminology</li>
<li>spell Checking</li>
</ul>
<p>Under pinning all this was the tokenization process which is where you chop up
each word (or combination of words, also called <code>n-grams</code>) into a single item.</p>
<p>Each main topic is discussed below.</p>
<div id="replacing-abbreviations-jargon-with-appropriate-terminology" class="section level4">
<h4>Replacing abbreviations, jargon with appropriate terminology</h4>
<p>As given in the example above, I created a list of common abbreviations and
jargon that were duplicates and also didn’t make a lot of sense on it’s own.<br />
This was tedious, slow and painful but once done provided a good step in
cleaning the data.</p>
<p>The function looked something like this:</p>
<pre class="r"><code>## function to find and replace strings 
#&#39; @param docVec vector where each element is a document
#&#39; @param replaceDf basically a lookup table with one column the text to look 
#&#39; for and the second column, the text to replace it with
#&#39; @param ignore_case option to ignore case in string matching
#&#39; Returns a character vector with the strings replaced appropriately
replaceTextFn &lt;- function(docVec, replaceDf, ignore_case = TRUE){
  output &lt;- docVec
  for (iR in 1:nrow(replaceDf)){
    abbrvStr &lt;- regex(paste0(&quot;\\b&quot;, replaceDf$abbreviation[iR], &quot;\\b&quot;), 
                      ignore_case = ignore_case)
    replaceStr &lt;- replaceDf$replacement_text[iR]
    
    output &lt;- str_replace_all(output, abbrvStr, replaceStr)
  }
  
  return(output)
}</code></pre>
<p>The function relies obviously on the <code>stringr</code> package.</p>
<p>As a side note, I did try using the <code>fastmatch::fmatch</code> function as suggested
in this <a href="https://stackoverflow.com/questions/41587158/replace-words-in-text2vec-efficiently">SO thread</a>
but found the timings to be quite similar. Quite possibly, this is because
of the loop I’m using.</p>
<p>Also, it is possible to try and replace all the terms in the corpus in this way
but I also found some diminishing returns for words that occur very infrequently.</p>
</div>
<div id="spell-checking" class="section level4">
<h4>Spell Checking</h4>
<p>While the method above is pretty good for words or abbreviations that are
commonly found in the corpus, it doesn’t really suit for spelling mistakes of
common words.</p>
<p>For example, here were five mispelled words in the corpus:</p>
<ul>
<li><code>goog</code>,</li>
<li><code>scondary</code>,</li>
<li><code>strenghts</code>,</li>
<li><code>thier</code>,</li>
<li><code>Accetable</code>,</li>
</ul>
<p>there were lots of spelling mistakes in the data. Trying to catch all word
replacements as in Step 1 would be unpractical as it would require constant
manual checking and updating.</p>
<p>To solve this problem, I used the <code>hunspell</code> spell checking package (which turns
out is also used in Firefox and RStudio). However, I also found that spell
checking for a domain specific corpus is not the same as general English usage.</p>
<p>Hence, I had to make two slight modifications to make this workable:</p>
<ol style="list-style-type: decimal">
<li>Introducing abbreviations to the dictionary</li>
<li>Prioritising words that are found in the corpus vocabulary instead of the
dictionary.</li>
</ol>
<p>Adding words to the dictionary is fairly straightforward:</p>
<p><code>myDict &lt;- dictionary(lang = "en_GB", add_words = dict$abbreviation)</code></p>
<p>Note that in this case, the <code>add_words</code> argument is just a list was manually
curated and included names and initials of people.</p>
<p>The <code>hunspell_suggest</code> function however, provides several possibilities of
spelling corrections. Some of these possibilities, taken blindly, actually
introduce words that are not in the vocabulary of the corpus. Take for example,
the misspelt word <code>"refr"</code>. Here are the suggested replacements from <code>hunspell</code>:</p>
<pre><code>## [[1]]
##  [1] &quot;ref&quot;   &quot;refer&quot; &quot;refs&quot;  &quot;rear&quot;  &quot;re fr&quot; &quot;re-fr&quot; &quot;ref r&quot; &quot;reef&quot;  &quot;freer&quot;
## [10] &quot;free&quot;</code></pre>
<p>The correct word replacement is <code>"refer"</code> which is the second word in the list.
The first word however, was not in the vocabulary of the corpus, hence, can be
removed. I thus, created a function that prioritised words in the corpus over
blindly using the suggested output of <code>hunspell</code>.</p>
<pre class="r"><code># customised spell checking function that prioritises words found in the corpus
# vocabulary instead of the output of hunspell_suggest
#&#39; @param x is a character string
spellCheckFn &lt;- function(x){
  ## get suggested spelling corrections
  tmp &lt;- unlist(hunspell_suggest(x, dict = myDict))
  
  ## rank them by term frequency as found in the corpus
  tmpVocab &lt;- vocab %&gt;% 
    filter(term %in% tmp) %&gt;% 
    arrange(desc(term_count))
  
  ## if suggested spellings in hunspell are found in corpus, use the 
  ## most frequent term, otherwise just use the first suggested item from 
  ## hunspell
  if (nrow(tmpVocab) &gt; 0){
    repWord &lt;- tmpVocab$term[1]
  } else{
    repWord &lt;- tmp[1]
  }
  return(repWord)
}</code></pre>
<p>Note that the dictionary <code>myDict</code> should be in the global environment and the
output of the function provides a single replacement text for the provided
input string.</p>
<p>Once replacements were found for each mispelt word, the corpus was then replaced
with the <code>replaceTextFn</code> above. The <code>vocab</code> object is a data frame containing
the term and frequency count of that term in the corpus–see below.</p>
</div>
<div id="tokenization" class="section level4">
<h4>Tokenization</h4>
<p>As a big fan of the <code>tidyverse</code>, I initially started using <a href="http://varianceexplained.org/about/">David Robinson’s</a>
<code>tidytext</code> package [1,2]. Tokenization with this package is very simple with
the <code>unnest_tokens</code> function and once ‘tidied’, was extremely natural to use
with all the common <code>dplyr</code> functions and (not)-pipes (i.e. <code>%&gt;%</code>). However, I
did notice that the tibbles it was creating were hogging up a lot of
memory.</p>
<p>Another package that that can be used for text analytics is <code>text2vec</code> [3,4]
which handles both the tokenization/DTM creation, and some very powerful
modelling methods as well. I found the tokenization quicker, and also very
memory efficient. It also handles <code>n-grams</code> extremely easily as well as
stop words. See [3,4] for more details.</p>
</div>
</div>
</div>
<div id="lda-packages" class="section level2">
<h2>LDA Packages</h2>
<p>One of the main reasons for using <code>text2vec</code> is because of its LDA modelling
capabilities and also has other in built embeddings like <em>GloVe</em> [5]. The LDA
model in <code>text2vec</code> is a wrapper to the <code>WarpLDA</code> algorithm [6] which I found
ran extremely efficiently. One potential downside is that the package is
relatively new with the latest CRAN version at 0.5.1. The package maintainer,
however, is really active so bugs do get fixed fairly quickly.</p>
<p>The other package I used was <code>topicmodels</code> [7] which provides more options but runs
significantly slower. The package works with the famous <code>tm</code> package so all
inputs will require conversion to <code>tm</code> objects. It does however, provide three
different topic modelling algorithms: one based on Gibbs sampling, one based on
variational expectation-maximisation (VEM) and correlated topics model (CTM).</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>[1] J. Silge and D. Robinson, “tidytext: Text Mining and Analysis Using
Tidy Data Principles in R.” <em>Journal Of Statistical Software</em>, vol. 1 no. 3,
2016. [Online]. Available:
<a href="http://dx.doi.org/10.21105/joss.00037">http://dx.doi.org/10.21105/joss.00037</a>.</p>
<p>[2] J. Silge and D. Robinson,“Text Mining with R”, [Online]. Available:
<a href="https://www.tidytextmining.com/tidytext.html">https://www.tidytextmining.com/tidytext.html</a></p>
<p>[3] D. Selivanov and Q. Wang, “text2vec: Modern Text Mining Framework for R”,
Available: <a href="https://cran.r-project.org/package=text2vec">https://cran.r-project.org/package=text2vec</a></p>
<p>[4] D. Selivanov, [Online]. Available: <a href="http://text2vec.org/">http://text2vec.org/</a></p>
<p>[5] J. Pennington, R. Socher, and C.D. Manning, “GloVe: Global Vectors for
Word Representation”, <em>Empirical Methods in Natural Language Processing (EMNLP)</em>,
pp. 1532-1543, 2014</p>
<p>[6] J. Chen, K. Li, J. Zhu, W. Chen, “WarpLDA: a Cache Efficient O(1)
Algorithm for Latent Dirichlet Allocation”, <em>ARXIV</em>, 2 Mar 2016, [Online].
Available: <a href="https://arxiv.org/abs/1510.08628">https://arxiv.org/abs/1510.08628</a></p>
<p>[7] B. Grün and K. Hornik, “topicmodels: An R Package for Fitting Topic Models.”
<em>Journal of Statistical Software</em> , vol. 40 no. 13, pp. 1-30. [Online].
Available: <a href="http://doi.org/10.18637/jss.v040.i13">http://doi.org/10.18637/jss.v040.i13</a></p>
</div>
